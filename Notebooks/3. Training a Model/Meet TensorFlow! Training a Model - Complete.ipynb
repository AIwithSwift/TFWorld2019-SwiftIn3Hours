{"nbformat":4,"nbformat_minor":0,"metadata":{"file_extension":".py","kernelspec":{"display_name":"Swift","language":"swift","name":"swift"},"language_info":{"file_extension":".swift","mimetype":"text/x-swift","name":"swift","version":""},"mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"colab":{"name":"Meet TensorFlow! Training a Model - Complete.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"qq_n--SKNanf","colab_type":"text"},"source":["#  Complete - Meet TensorFlow! Training a XOR Model"]},{"cell_type":"markdown","metadata":{"id":"16937WSdNank","colab_type":"text"},"source":["In this example, we assemble a multilayer peceptron network that can perform XOR. \n","\n","It's not very useful, but it showcases how you build up a model using layers, and how to execute training with that model. \n","\n","It's simple enough that you know whether it's correct... which is why we're doing it!\n"]},{"cell_type":"markdown","metadata":{"id":"jJZxT8xyTDXC","colab_type":"text"},"source":["## Setting up"]},{"cell_type":"markdown","metadata":{"id":"2bRvS7ipNsAf","colab_type":"text"},"source":["First, we need to `import` the TensorFlow framework:"]},{"cell_type":"code","metadata":{"id":"PbMmpBLrNann","colab_type":"code","colab":{}},"source":["import TensorFlow"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L7nCrdjhN0hv","colab_type":"text"},"source":["## Creating the model\n","\n","To represent our XOR neural network model, we need to create a `struct`, adhering to the  [`Layer` Protocol](https://www.tensorflow.org/swift/api_docs/Protocols/Layer) (which is part of Swift For TensorFlow's API). Ours is called `XORModel`.\n","\n","Inside the model, we want three layers:\n","* an input layer, to take the input\n","* a hidden layer \n","* an output layer, to provide the output\n","\n","All three layers should be a `Dense` layer (a [densely-connected layer](https://www.tensorflow.org/swift/api_docs/Structs/Dense)) that takes an `inputSize` and an `outputSize`. \n","\n","The `inputSize` specifies that the input to the layer is of that many values. Likewise `outputSize`, for the out of the layer.\n","\n","Each will have an activation using an `activation` function determines the output shape of each node in the layer. There are many available activations, but [ReLU](https://www.tensorflow.org/swift/api_docs/Functions#leakyrelu_:alpha:) and [Sigmoid](https://www.tensorflow.org/swift/api_docs/Functions#sigmoid_:) are common. \n","\n","For our three layers, we'll use `sigmoid`.\n","\n","We'll also need to provide a definition of our `@differentiable` `func`, `callAsFunction()`. In this case, we want it to return the `input` sequenced through (passed through) the three layers. \n","\n","Helpfully, the `Differentiable` `protocol` that comes with Swift for TensorFlow has a method, [`sequenced()`](https://www.tensorflow.org/swift/api_docs/Protocols/Differentiable#sequencedthrough:_:) that makes this trivial.\n","\n"]},{"cell_type":"code","metadata":{"id":"2812LRBTNant","colab_type":"code","colab":{}},"source":["// Create a XORModel Struct\n","struct XORModel: Layer\n","{\n","  // define three layers, each of Dense type\n","  var inputLayer = Dense<Float>(inputSize: 2, outputSize: 2, activation: sigmoid)\n","  var hiddenLayer = Dense<Float>(inputSize: 2, outputSize: 2, activation: sigmoid)\n","  var outputLayer = Dense<Float>(inputSize: 2, outputSize: 1, activation: sigmoid)\n","  \n","  // provide the differentiable thingo\n","  @differentiable func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float>\n","  {\n","    return input.sequenced(through: inputLayer, hiddenLayer, outputLayer)\n","  }\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ybin-B0Qy2o","colab_type":"text"},"source":["## Creating an instance of our model"]},{"cell_type":"markdown","metadata":{"id":"yKjOCu_RRBPM","colab_type":"text"},"source":["Here we need to create an instance of our XORModel Struct, which we defined above. This will be our model."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kZRlD4utdPuX","colab":{}},"source":["var model = XORModel()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hxDDiGpXQ3DQ","colab_type":"text"},"source":["## Creating an optimizer"]},{"cell_type":"markdown","metadata":{"id":"blHjk_IpRLV9","colab_type":"text"},"source":["And we need an [optimiser](https://www.tensorflow.org/swift/api_docs/Protocols/Optimizer), in this case we're going to use [stochastic gradient descent (SGD) optimiser](https://www.tensorflow.org/swift/api_docs/Classes/SGD), which we can get from the Swift for TensorFlow library.\n","\n","Our optimiser is, obviously, for the model instance we defined a moment ago, and wants a learning rate of about 0.02."]},{"cell_type":"code","metadata":{"id":"op5PRWVoQ2iu","colab_type":"code","colab":{}},"source":["let optimiser = SGD(for: model, learningRate: 0.02)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vlm3A-FKQ8MB","colab_type":"text"},"source":["##  Creating and labelling training data\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Gqn5RlZaRvrh","colab_type":"text"},"source":["We need an array of type [`Tensor`](https://www.tensorflow.org/swift/api_docs/Structs/Tensor) to hold our training data (`[0, 0], [0, 1], [1, 0], [1, 1]`):"]},{"cell_type":"code","metadata":{"id":"zjyQoNqnQ7t4","colab_type":"code","colab":{}},"source":["let trainingData: Tensor<Float> = [[0, 0], [0, 1], [1, 0], [1, 1]]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ttw6mJUSJZc","colab_type":"text"},"source":["And we need to label the training data so that we know the correct outputs:\n"]},{"cell_type":"code","metadata":{"id":"KBRwcj0MSKX5","colab_type":"code","colab":{}},"source":["let trainingLabels: Tensor<Float> = [[0], [1], [1], [0]]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nmq98WiPSQE9","colab_type":"text"},"source":["## Training the model"]},{"cell_type":"markdown","metadata":{"id":"dieIbtmdVvgB","colab_type":"text"},"source":["First, we need a hyperparameter for epochs:"]},{"cell_type":"code","metadata":{"id":"CDt6kEjKVyWs","colab_type":"code","colab":{}},"source":["let epochs = 100_000"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8zAz5DDiWVM7","colab_type":"text"},"source":["Then we need a training loop. We train the model by iterating through our epochs, and each time update the gradient (the ùõÅ symbol, nabla, is often used to represent gradient). Our gradient is of type [`TangentVector`](https://www.tensorflow.org/swift/api_docs/Protocols/Differentiable#tangentvector), and represents a differentiable value‚Äôs derivatives.\n","\n","Each epoch, we set the predicted value to be our training data, and the expected value to be our training data, and calculate the loss using [`meanSquaredError()`](https://www.tensorflow.org/swift/api_docs/Functions#meansquarederrorpredicted:expected:).\n","\n","Every so often we also want to print out the epoch we're in, and the current loss, so we can watch the traning. We also need to return loss.\n","\n","Finally, we need to use our [optimizer](https://www.tensorflow.org/swift/api_docs/Protocols/Optimizer) to [update](https://www.tensorflow.org/swift/api_docs/Protocols/Optimizer#update_:along:) the differentiable variables, along the gradient.\n"]},{"cell_type":"code","metadata":{"id":"8QGopdwKNan3","colab_type":"code","outputId":"ba04d754-9e6b-49a4-a77b-0cd5c14488c2","executionInfo":{"status":"ok","timestamp":1572299228737,"user_tz":240,"elapsed":375992,"user":{"displayName":"Paris B-A","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mATPYgh7Oradnt-89Tt0-SyerXDc8Z985jmLS2U=s64","userId":"01326454337734597812"}},"colab":{"base_uri":"https://localhost:8080/","height":371}},"source":["for epoch in 0..<epochs\n","{\n","    // closure for the gradient\n","    let ùõÅmodel = model.gradient { model -> Tensor<Float> in\n","\n","        // predicted value (the training data)\n","        let ≈∑ = model(trainingData)\n","\n","        // loss \n","        let loss = meanSquaredError(predicted: ≈∑, expected: trainingLabels)\n","\n","        // sometimes we want to print an update\n","        if epoch % 5000 == 0\n","        {\n","          print(\"epoch: \\(epoch) loss: \\(loss)\")\n","        }\n","        return loss\n","    }\n","    // update the model\n","    optimiser.update(&model, along: ùõÅmodel)\n","}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch: 0 loss: 0.28061706\n","epoch: 5000 loss: 0.24945608\n","epoch: 10000 loss: 0.2490734\n","epoch: 15000 loss: 0.24835764\n","epoch: 20000 loss: 0.2469819\n","epoch: 25000 loss: 0.24420786\n","epoch: 30000 loss: 0.23800798\n","epoch: 35000 loss: 0.2237089\n","epoch: 40000 loss: 0.20193933\n","epoch: 45000 loss: 0.18637396\n","epoch: 50000 loss: 0.17841955\n","epoch: 55000 loss: 0.17337665\n","epoch: 60000 loss: 0.16862151\n","epoch: 65000 loss: 0.1626256\n","epoch: 70000 loss: 0.15436587\n","epoch: 75000 loss: 0.14475374\n","epoch: 80000 loss: 0.1311733\n","epoch: 85000 loss: 0.056418493\n","epoch: 90000 loss: 0.02172982\n","epoch: 95000 loss: 0.012243575\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fwskbFpjNan7","colab_type":"text"},"source":["## Testing the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uSC0vIJVv_Cj","outputId":"8f6ff5d2-f601-4a1b-cf4e-c126bc09a4c5","executionInfo":{"status":"ok","timestamp":1572299229757,"user_tz":240,"elapsed":377007,"user":{"displayName":"Paris B-A","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mATPYgh7Oradnt-89Tt0-SyerXDc8Z985jmLS2U=s64","userId":"01326454337734597812"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["print(round(model.inferring(from: [[0, 0], [0, 1], [1, 0], [1, 1]])))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0.0],\r\n"," [1.0],\r\n"," [1.0],\r\n"," [0.0]]\r\n"],"name":"stdout"}]}]}